{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re             # regular expression\n",
    "import pickle         # Serialization and de-serialization of object (saving and loading)\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' #LIMITS the lines of waring upto 3 lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nINPUT -> Encoder -> ENC OUTPUTS, THOUGHT VECTOR -> Attention Network -> Attention Weights (x ENC OUTPUTS) -> ATTENTION OUTPUT\\n\\nATTENTION OUTPUT, PREV DECODER STATE -> DECODER ->FINAL OUTPUT\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#encoder and decoder architecture\n",
    "\"\"\"\n",
    "INPUT -> Encoder -> ENC OUTPUTS, THOUGHT VECTOR -> Attention Network -> Attention Weights (x ENC OUTPUTS) -> ATTENTION OUTPUT\n",
    "\n",
    "ATTENTION OUTPUT, PREV DECODER STATE -> DECODER ->FINAL OUTPUT\n",
    "\"\"\"\n",
    "# LSTM take more time and used for large datasets.\n",
    "# where as GRU is good for small dataset and faster.\n",
    "\n",
    "# Attention architecture layerclass (Neural network) - a simple dense layer\n",
    "# Attention Network -> Attention Weights (x ENC OUTPUTS) -> ATTENTION OUTPUT\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding, encoder_units, batch_size ):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.enc_units = encoder_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding)\n",
    "        self.gru = tf.keras.layers.GRU(self.enc_units, return_sequences = True, return_state = True, kernel_regularizer =tf.keras.regularizers.L2(0.001))\n",
    "    \n",
    "    def call(self, inputs, hidden_state):\n",
    "        embedded_input = self.embedding(inputs)\n",
    "        enc_outputs, thought_vector = self.gru(embedded_input, initial_state = hidden_state)\n",
    "        return enc_outputs, thought_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the two inputs are enc_outputs and the thought_vector going into the attention layer\n",
    "class Attention(tf.keras.layers.Layer ):\n",
    "    def __init__(self, units):\n",
    "        self.enc_output_layer = tf.keras.layers.Dense(units, kernel_regularizer =tf.keras.regularizers.L2(0.001))\n",
    "        self.thought_layer    = tf.keras.layers.Dense(units, kernel_regularizer =tf.keras.regularizers.L2(0.001))\n",
    "        self.final_layer      = tf.keras.layers.Dense(1    , kernel_regularizer =tf.keras.regularizers.L2(0.001))\n",
    "        \n",
    "    def call(self, enc_outputs, thought_vector):\n",
    "        thought_matrix = tf.expand.dims(thought_vector, 1)\n",
    "        scores = self.final_layer(tf.keras.activations.tanh(self.enc_output_layer(enc_outputs) + self.thought_layer(thought_matrix)))\n",
    "        attention_weights = tf.keras.activations.softmax(scores, axis=-1)\n",
    "        \n",
    "        attention_output = attention_weights * enc_outputs          # shape(batch_size,num_output,output_size)\n",
    "        attention_output = tf.reduce_sum(attention_output, axis=1)  # New shape (batch_size, output_size)\n",
    "        \n",
    "        return attention_output, attention_weights\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding, decoder_units, batch_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.dec_units = decoder_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding)\n",
    "        self.gru = tf.keras.layers.GRU(self.dec_units, return_sequences = True, return_state = True, kernel_regularizer =tf.keras.regularizers.L2(0.001))\n",
    "        \n",
    "        self.attention = Attention(self.dec_units)\n",
    "        self.word_output = tf.keras.layers.Dense(vocab_size, kernel_regularizer =tf.keras.regularizers.L2(0.001))\n",
    "        \n",
    "    def call(self, inputs, enc_outputs, thought_vector):\n",
    "        attention_output, attention_weights = self.attention(enc_outputs, thought_vector)\n",
    "        \n",
    "        # Shape of attention output (batch_size, size_of_embedding)\n",
    "        \n",
    "        embedded_inputs = self.embedding(inputs) # shape (batch_size ,num_words, size_of_embedding) \n",
    "        attention_output = tf.expand_dims(attention_outputs, 1) #  shape of attention output (batch_size, size_of_embedding)\n",
    "        concat_inputs = tf.concat([attention_output, embedded_inputs], axis=-1)\n",
    "        \n",
    "        decoder_outputs, hidden_state = self.gru(concat_inputs)\n",
    "        decoder_outputs = tf.reshape(decoder_outputs, (-1,decoder_outputs.shape[2]))\n",
    "        \n",
    "        final_outputs = self.word_output(decoder_outputs)\n",
    "        \n",
    "        return final_outputs, hidden_state, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Train:\n",
    "    def __init__(self):\n",
    "        self.optimizer = tf.keras.optimizers.Adem()\n",
    "        self.loss_function = tf.keras"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
