{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re             # regular expression\n",
    "import pickle         # Serialization and de-serialization of object (saving and loading)\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' #LIMITS the lines of waring upto 3 lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoder and decoder architecture\n",
    "\"\"\"\n",
    "INPUT -> Encoder -> ENC OUTPUTS, THOUGHT VECTOR -> Attention Network -> Attention Weights (x ENC OUTPUTS) -> ATTENTION OUTPUT\n",
    "\n",
    "ATTENTION OUTPUT, PREV DECODER STATE -> DECODER ->FINAL OUTPUT\n",
    "\"\"\"\n",
    "# LSTM take more time and used for large datasets.\n",
    "# where as GRU is good for small dataset and faster.\n",
    "\n",
    "# Attention architecture layerclass (Neural network) - a simple dense layer\n",
    "# Attention Network -> Attention Weights (x ENC OUTPUTS) -> ATTENTION OUTPUT\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding, encoder_units, batch_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.enc_units = encoder_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding)\n",
    "        self.gru = tf.keras.layers.GRU(self.enc_units, return_sequences=True, return_state=True, recurrent_initializer='glorot_uniform', kernel_regularizer=tf.keras.regularizers.L2(0.001))\n",
    "    \n",
    "    def call(self, inputs, hidden_state):\n",
    "        embedded_inputs = self.embedding(inputs)\n",
    "        enc_outputs, thought_vector = self.gru(embedded_inputs, initial_state=hidden_state)\n",
    "        return enc_outputs, thought_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the two inputs are enc_outputs and the thought_vector going into the attention layer\n",
    "class Attention(tf.keras.layers.Layer ):\n",
    "    def __init__(self, units):\n",
    "        self.enc_output_layer = tf.keras.layers.Dense(units, kernel_regularizer =tf.keras.regularizers.L2(0.001))\n",
    "        self.thought_layer = tf.keras.layers.Dense(units, kernel_regularizer =tf.keras.regularizers.L2(0.001))\n",
    "        self.final_laye = tf.keras.layers.Dense(1    , kernel_regularizer =tf.keras.regularizers.L2(0.001))\n",
    "        \n",
    "    def call(self, enc_outputs, thought_vector):\n",
    "        thought_matrix = tf.expand.dims(thought_vector, 1)\n",
    "        scores = self.final_layer(tf.keras.activations.tanh(self.enc_output_layer(enc_outputs) + self.thought_layer(thought_matrix)))\n",
    "        attention_weights = tf.keras.activations.softmax(scores, axis=-1)\n",
    "        \n",
    "        attention_output = attention_weights * enc_outputs          # shape(batch_size,num_output,output_size)\n",
    "        attention_output = tf.reduce_sum(attention_output, axis=1)  # New shape (batch_size, output_size)\n",
    "        \n",
    "        return attention_output, attention_weights\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding, decoder_units, batch_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.dec_units = decoder_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding)\n",
    "        self.gru = tf.keras.layers.GRU(self.dec_units, return_sequences = True, return_state = True, kernel_regularizer =tf.keras.regularizers.L2(0.001))\n",
    "        \n",
    "        self.attention = Attention(self.dec_units)\n",
    "        self.word_output = tf.keras.layers.Dense(vocab_size, kernel_regularizer =tf.keras.regularizers.L2(0.001))\n",
    "        \n",
    "    def call(self, inputs, enc_outputs, thought_vector):\n",
    "        attention_output, attention_weights = self.attention(enc_outputs, thought_vector)\n",
    "        \n",
    "        # Shape of attention output (batch_size, size_of_embedding)\n",
    "        \n",
    "        embedded_inputs = self.embedding(inputs) # shape (batch_size ,num_words, size_of_embedding) \n",
    "        attention_output = tf.expand_dims(attention_outputs, 1) #  shape of attention output (batch_size, size_of_embedding)\n",
    "        concat_inputs = tf.concat([attention_output, embedded_inputs], axis=-1)\n",
    "        \n",
    "        decoder_outputs, hidden_state = self.gru(concat_inputs)\n",
    "        decoder_outputs = tf.reshape(decoder_outputs, (-1,decoder_outputs.shape[2]))\n",
    "        \n",
    "        final_outputs = self.word_output(decoder_outputs)\n",
    "        \n",
    "        return final_outputs, hidden_state, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Train:\n",
    "    def __init__(self):\n",
    "        self.optimizer = tf.keras.optimizers.Adam()\n",
    "        self.base_loss_function = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "        \n",
    "    def loss_function(self, y_real, y_pred):\n",
    "        base_mask = tf.math.logical_not(tf.math.equal(y_real, 0))\n",
    "        base_loss = self.base_loss_function(y_real, y_pred)\n",
    "        mask = tf.cast(base_mask, dtype=base_loss.dtype)\n",
    "        final_loss = mask * base_loss\n",
    "        return tf.reduce_mean(final_loss)\n",
    "    \n",
    "    def train_step(self, train_data, label_data, enc_hidden, encoder, decoder, batch_size, label_tokenizer):\n",
    "        loss = 0\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            enc_outputs, thought_vector = encoder(train_data, enc_hidden)\n",
    "            dec_hidden = thought_vector\n",
    "            dec_input = tf.expand_dims([label_tokenizer.word_index['<start>']] * batch_size, 1)\n",
    "            \n",
    "            for index in range(1, label_data.shape[1]):\n",
    "                outputs, dec_hidden, _ = decoder(dec_input, enc_outputs, dec_hidden)\n",
    "                dec_input = tf.expand_dims(label_data[:, index], 1)\n",
    "                loss = loss + self.loss_function(label_data[:, index], outputs)\n",
    "        \n",
    "        word_loss = loss / int(label_data.shape[1])\n",
    "        variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "        gradients = tape.gradient(loss, variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, variables))\n",
    "        \n",
    "        return word_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data_Preprocessing:\n",
    "    def __init__(self):\n",
    "        self.temp = None\n",
    "    \n",
    "    def get_data(self, path):\n",
    "        file = open(path, 'r').read()\n",
    "        lists = [f.split('\\t') for f in file.split('\\n')]\n",
    "        questions = [x[0] for x in lists]\n",
    "        answers = [x[1] for x in lists]\n",
    "        return questions, answers\n",
    "    \n",
    "    def process_sentence(self, line):\n",
    "        line = line.lower().strip()\n",
    "        line = re.sub(r\"([?!.,])\", r\" \\1 \", line)\n",
    "        line = re.sub(r'[\" \"]+', \" \", line)\n",
    "        line = re.sub(r\"[^a-zA-Z?!.,]+\", \" \", line)\n",
    "        line = line.strip()\n",
    "        line = '<start> ' + line + ' <end>'\n",
    "        return line\n",
    "    \n",
    "    def word_to_vec(self, inputs):\n",
    "        tokenizer = Tokenizer(filters='')\n",
    "        tokenizer.fit_on_texts(inputs)\n",
    "        vectors = tokenizer.texts_to_sequences(inputs)\n",
    "        vectors = pad_sequences(vectors, padding='post')\n",
    "        \n",
    "        return vectors, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Data_Preprocessing()\n",
    "\n",
    "questions, answers = data.get_data('chatbot.txt')\n",
    "questions = [data.process_sentence(str(sentence)) for sentence in questions]\n",
    "answers = [data.process_sentence(str(sentence)) for sentence in answers]\n",
    "train_vectors, train_tokenizer = data.word_to_vec(questions)\n",
    "label_vectors, label_tokenizer = data.word_to_vec(answers)\n",
    "max_length_train = train_vectors.shape[1]\n",
    "max_length_label = label_vectors.shape[1]\n",
    "batch_size = 64\n",
    "buffer_size = train_vectors.shape[0]\n",
    "embedding_dim = 256\n",
    "steps_per_epoch = buffer_size//batch_size\n",
    "units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_train = len(train_tokenizer.word_index) + 1\n",
    "vocab_label = len(label_tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((train_vectors, label_vectors))\n",
    "dataset = dataset.shuffle(buffer_size)\n",
    "dataset = dataset.batch(batch_size, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(vocab_train, embedding_dim, units, batch_size)\n",
    "decoder = Decoder(vocab_label, embedding_dim, units, batch_size)\n",
    "trainer = Train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 20\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    enc_hidden = tf.zeros((batch_size, units))\n",
    "    total_loss = 0\n",
    "    \n",
    "    for (batch_num, (train_data, label_data)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "        batch_loss = trainer.train_step(train_data, label_data, enc_hidden, encoder, decoder, batch_size, label_tokenizer)\n",
    "        total_loss = total_loss + batch_loss\n",
    "        \n",
    "    print(f\"Epoch: {epoch}, Loss: {total_loss/steps_per_epoch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Chatbot:\n",
    "    def __init__(self, encoder, decoder, train_tokenizer, label_tokenizer, max_length_train, units):\n",
    "        self.train_tokenizer = train_tokenizer\n",
    "        self.label_tokenizer = label_tokenizer\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.units = units\n",
    "        self.data = Data_Preprocessing()\n",
    "        self.maxlen = max_length_train\n",
    "    \n",
    "    def clean_answer(self, answer):\n",
    "        answer = answer[:-1]\n",
    "        answer = ' '.join(answer)\n",
    "        return answer\n",
    "    \n",
    "    def predict(self, sentence):\n",
    "        sentence = self.data.process_sentence(sentence)\n",
    "        \n",
    "        sentence_mat = []\n",
    "        for word in sentence.split(\" \"):\n",
    "            try:\n",
    "                sentence_mat.append(self.train_tokenizer.word_index[word])\n",
    "            except:\n",
    "                return \"Could not understand that, can you re-phrase?\"\n",
    "        \n",
    "        sentence_mat = pad_sequences([sentence_mat], maxlen=self.maxlen, padding='post')\n",
    "        sentence_mat = tf.convert_to_tensor(sentence_mat)\n",
    "        enc_hidden = [tf.zeros((1, self.units))]\n",
    "        encoder_outputs, thought_vector = self.encoder(sentence_mat, enc_hidden)\n",
    "        dec_hidden = thought_vector\n",
    "        dec_input = tf.expand_dims([label_tokenizer.word_index['<start>']], 0)\n",
    "        \n",
    "        answer = []\n",
    "        for i in range(1, self.maxlen):\n",
    "            pred, dec_hidden, _ = decoder(dec_input, encoder_outputs, dec_hidden)\n",
    "            word = self.label_tokenizer.index_word[np.argmax(pred[0])]\n",
    "            answer.append(word)\n",
    "            \n",
    "            if word == '<end>':\n",
    "                return self.clean_answer(answer)\n",
    "            \n",
    "            dec_input = tf.expand_dims([np.argmax(pred[0])], 0)\n",
    "        \n",
    "        return self.clean_answer(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bot = Chatbot(encoder, decoder, train_tokenizer, label_tokenizer, max_length_train, units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = ''\n",
    "while True:\n",
    "    question = str(input('You:'))\n",
    "    if question == 'quit' or question == 'Quit':\n",
    "        break\n",
    "        \n",
    "    answer = bot.predict(question)\n",
    "    print(f'Bot: {answer}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = [[1,2,3,4,5]]\n",
    "pred[0] = [1,2,3,4,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence - english sentence\n",
    "\n",
    "remove things from it \n",
    "convert it to one hot form\n",
    "pass it through the whole model and get the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
